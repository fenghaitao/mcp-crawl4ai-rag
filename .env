# MCP Server Configuration
# The transport for the MCP server - either 'sse' or 'stdio' (defaults to sse if left empty)
TRANSPORT=sse

# Host to bind to if using sse as the transport (leave empty if using stdio)
# Set this to 0.0.0.0 if using Docker, otherwise set to localhost (if using uv)
HOST=0.0.0.0

# Port to listen on if using sse as the transport (leave empty if using stdio)
PORT=8051

# AI Provider Configuration
# Choose between OpenAI or GitHub Copilot for embeddings and chat
USE_QWEN_EMBEDDINGS=false
USE_QWEN_RERANKER=false
USE_COPILOT_EMBEDDINGS=true
USE_COPILOT_CHAT=true

# DashScope API Configuration (for code summarization with Qwen models via LiteLLM)
# Get your API key from https://dashscope.console.aliyun.com/
# Used for code summarization with qwen3-coder-plus model via LiteLLM
# LiteLLM uses the dashscope/ provider prefix
IFLOW_API_KEY=sk-1e4320e56afcb4032bb22d93cbda9eb5

# Rate Limiting Configuration for DashScope API
# Adjust based on your usage patterns and API limits
DASHSCOPE_REQUESTS_PER_MINUTE=60
DASHSCOPE_BURST_LIMIT=10

# Code Summarization Configuration
# USE_CODE_SUMMARIZATION: Enables file-level and chunk-level summarization for better search
# Uses DashScope qwen3-coder-plus model (via LiteLLM) for Simics-specific code understanding
USE_CODE_SUMMARIZATION=true

# OpenAI API Configuration (used when USE_COPILOT_EMBEDDINGS=false or USE_COPILOT_CHAT=false)
# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# This is for the embedding model - text-embedding-3-small will be used
OPENAI_API_KEY=your_openai_api_key_here

# GitHub Copilot Configuration (used when USE_COPILOT_EMBEDDINGS=true or USE_COPILOT_CHAT=true)
# Get your GitHub token from https://github.com/settings/tokens
# Requires Copilot subscription and appropriate permissions
#GITHUB_TOKEN=your_github_token_here

# Rate Limiting Configuration for Copilot API
# Adjust based on your usage patterns and API limits
COPILOT_REQUESTS_PER_MINUTE=60

# The LLM you want to use for summaries and contextual embeddings
# Options: gpt-4o-mini (OpenAI), gpt-4o (Copilot), gpt-4.1-nano (if available)
MODEL_CHOICE=gpt-4o

# RAG strategies - set these to "true" or "false" (default to "false")
# USE_CONTEXTUAL_EMBEDDINGS: Enhances embeddings with contextual information for better retrieval
USE_CONTEXTUAL_EMBEDDINGS=true

# USE_HYBRID_SEARCH: Combines vector similarity search with keyword search for better results
USE_HYBRID_SEARCH=true

# USE_AGENTIC_RAG: Enables code example extraction, storage, and specialized code search functionality
USE_AGENTIC_RAG=true

# USE_RERANKING: Applies cross-encoder reranking to improve search result relevance
USE_RERANKING=true

# USE_KNOWLEDGE_GRAPH: Enables AI hallucination detection and repository parsing tools using Neo4j
# If you set this to true, you must also set the Neo4j environment variables below.
USE_KNOWLEDGE_GRAPH=true

# Database Backend Configuration
# Choose your database backend: 'supabase' (default) or 'chroma'
# - supabase: PostgreSQL with pgvector for production use
# - chroma: Local vector database for development (no setup required)
DB_BACKEND=supabase

# Supabase Configuration (required if DB_BACKEND=supabase)
# Get these from your Supabase project settings -> API
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# Cloud Supabase Configuration (production - commented out)
#SUPABASE_URL=https://hkeummfhryxuvhfmoeqo.supabase.co
#SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhrZXVtbWZocnl4dXZoZm1vZXFvIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1OTY1ODQ5MiwiZXhwIjoyMDc1MjM0NDkyfQ.b5TjIGUYWDqmlmfjPd-v6vVuAOE8GlEDtVfm8wkUoYo
#SUPABASE_URL=https://ligqfmoqutbrivnhnkal.supabase.co
#SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImxpZ3FmbW9xdXRicml2bmhua2FsIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2MDExNDc3MywiZXhwIjoyMDc1NjkwNzczfQ.nSnAuMtcTQZq3SlR02vXn3DXi__kMhUYOzR4tl7xxs4

# Local Supabase Configuration (development)
SUPABASE_URL=http://localhost:8000
SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q

# ChromaDB Configuration (required if DB_BACKEND=supabase)
# Path to store ChromaDB data (default: ./chroma_db)
CHROMA_DB_PATH=./chroma_db

# Neo4j Configuration for Knowledge Graph Tools
# These are required for the AI hallucination detection and repository parsing tools
# Leave empty to disable knowledge graph functionality

# Neo4j connection URI - use bolt://localhost:7687 for local, neo4j:// for cloud instances
# IMPORTANT: If running the MCP server through Docker, change localhost to host.docker.internal
NEO4J_URI=neo4j+s://d2bb776e.databases.neo4j.io

# Neo4j username (usually 'neo4j' for default installations)
NEO4J_USER=neo4j

# Neo4j password for your database instance
NEO4J_PASSWORD=Ys5uOQXJllVlIE1hT9d1artIdeu3S9TAogrYDsFgsww

# Web Crawling Content Configuration
# CRAWL_STATIC_CONTENT_ONLY: When true, crawls only static HTML content without JavaScript execution
# This prevents redirects and gives you the original page content (recommended for most documentation)
# Set to false for dynamic content that requires JavaScript to load properly
CRAWL_STATIC_CONTENT_ONLY=true

# Simics Source Code Integration
# CRAWL_SIMICS_SOURCE: Enable crawling of Simics DML and Python source code
CRAWL_SIMICS_SOURCE=true

# SIMICS_SOURCE_PATH: Path to the Simics packages directory
SIMICS_SOURCE_PATH=simics-7-packages-2025-38-linux64/

# User Manual Chunker Configuration
# These settings control how documentation is chunked for RAG

# MANUAL_MAX_CHUNK_SIZE: Maximum size of each chunk in characters (default: 1000)
MANUAL_MAX_CHUNK_SIZE=2000

# MANUAL_MIN_CHUNK_SIZE: Minimum size of each chunk in characters (default: 100)
MANUAL_MIN_CHUNK_SIZE=100

# MANUAL_CHUNK_OVERLAP: Number of overlapping characters between chunks (default: 50)
MANUAL_CHUNK_OVERLAP=50

# MANUAL_SIZE_METRIC: Metric for measuring chunk size - 'characters' or 'tokens' (default: characters)
MANUAL_SIZE_METRIC=characters

# MANUAL_EMBEDDING_MODEL: Model to use for generating embeddings (default: text-embedding-3-small)
MANUAL_EMBEDDING_MODEL=text-embedding-3-small

# MANUAL_SUMMARY_MODEL: Model to use for generating summaries (default: iflow/qwen3-coder-plus)
MANUAL_SUMMARY_MODEL=iflow/qwen3-coder-plus

# MANUAL_GENERATE_SUMMARIES: Whether to generate summaries for chunks (default: true)
MANUAL_GENERATE_SUMMARIES=true

# MANUAL_GENERATE_EMBEDDINGS: Whether to generate embeddings for chunks (default: true)
MANUAL_GENERATE_EMBEDDINGS=true
