"""
RAG pipeline CLI commands.

This module provides commands for RAG operations including:
- Document ingestion and processing
- Content chunking with embeddings
- Semantic search and querying
- File and chunk management
"""

import click
from typing import Optional
from pathlib import Path

from .utils import handle_cli_errors, verbose_echo


@click.group()
def rag():
    """RAG pipeline operations (ingestion, egestion, querying)."""
    pass


@rag.command()
@click.argument('query_text')
@click.option('--limit', '-l', type=int, default=5, help='Number of results to return')
@click.option('--threshold', '-t', type=float, default=None, 
              help='Similarity threshold for results (default: no threshold)')
@click.option('--content-type', type=click.Choice(['documentation', 'code_dml', 'python_test']),
              help='Filter by content type')
@click.option('--json', 'json_output', is_flag=True, help='Output in JSON format')
@click.pass_context
@handle_cli_errors
def query(ctx, query_text: str, limit: int, threshold: Optional[float], content_type: Optional[str], json_output: bool):
    """Query the RAG system with semantic search."""
    from ..backends.factory import get_backend
    import json
    
    verbose_echo(ctx, f"Querying RAG system: {query_text}")
    
    if not json_output:
        click.echo(f"üîç Query: {query_text}")
        click.echo(f"üìä Limit: {limit} results")
        if threshold is not None:
            click.echo(f"üéØ Threshold: {threshold}")
        if content_type:
            click.echo(f"üìÅ Content Type: {content_type}")
        click.echo()
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        if not json_output:
            click.echo("üöÄ Executing semantic search...")
        
        # Perform semantic search
        results = backend.semantic_search(
            query_text=query_text,
            limit=limit,
            content_type=content_type,
            threshold=threshold
        )
        
        if json_output:
            # JSON output
            output = {
                'query': query_text,
                'limit': limit,
                'threshold': threshold,
                'content_type': content_type,
                'result_count': len(results),
                'results': [
                    {
                        'chunk_id': r.get('id'),
                        'file_id': r.get('file_id'),
                        'url': r.get('url', ''),
                        'chunk_number': r.get('chunk_number', 0),
                        'similarity': r.get('similarity', 0),
                        'summary': r.get('summary', ''),
                        'content_preview': r.get('content', '')[:200] + '...' if len(r.get('content', '')) > 200 else r.get('content', ''),
                        'metadata': r.get('metadata', {})
                    }
                    for r in results
                ]
            }
            click.echo(json.dumps(output, indent=2))
        else:
            # Human-readable output
            if not results:
                click.echo("üì≠ No results found matching your query.")
            else:
                click.echo(f"\nüìÑ Found {len(results)} result(s):")
                click.echo("=" * 80)
                
                for i, result in enumerate(results, 1):
                    click.echo(f"\nüî∏ Result #{i}")
                    click.echo(f"   üìä Similarity: {result.get('similarity', 0):.4f}")
                    click.echo(f"   üìç Source: {result.get('url', 'Unknown')}")
                    click.echo(f"   üì¶ Chunk #{result.get('chunk_number', 'Unknown')}")
                    
                    summary = result.get('summary', '')
                    if summary:
                        click.echo(f"   üìù Summary: {summary}")
                    
                    content = result.get('content', '')
                    if content:
                        # Show first 300 characters
                        if len(content) > 300:
                            content = content[:300] + "..."
                        click.echo(f"   üìÑ Content: {content}")
                    
                    metadata = result.get('metadata', {})
                    if metadata:
                        # Show relevant metadata fields
                        relevant_fields = ['title', 'section', 'word_count', 'has_code']
                        relevant_meta = {k: v for k, v in metadata.items() if k in relevant_fields}
                        if relevant_meta:
                            click.echo(f"   üè∑Ô∏è  Metadata: {json.dumps(relevant_meta, indent=6)}")
                
                click.echo("\n" + "=" * 80)
                click.echo(f"‚úÖ Query completed successfully! Found {len(results)} results.")
        
    except Exception as e:
        click.echo(f"‚ùå Error during query: {e}", err=True)
        import traceback
        traceback.print_exc()
        raise


@rag.command()
@click.argument('file_path', type=click.Path(exists=True))
@click.option('-f', '--force', is_flag=True, help='Force re-processing (remove existing data and re-ingest)')
@click.pass_context
@handle_cli_errors
def ingest_dml(ctx, file_path: str, force: bool):
    """Ingest a DML source code file."""
    verbose_echo(ctx, "Ingesting DML source file...")
    
    click.echo(f"üìÑ DML file: {file_path}")
    click.echo(f"üîÑ Force re-processing: {'Yes' if force else 'No'}")
    
    try:
        if force:
            click.echo("üóëÔ∏è Force mode: Removing existing data for this file...")
            # TODO: Egest/remove existing data for this specific file from database
        
        click.echo("üöÄ Starting DML file ingestion...")
        click.echo("üìã Processing .dml file...")
        click.echo("üß† Generating embeddings...")
        click.echo("üíæ Storing in database...")
        
        # TODO: Integrate with actual DML file processing logic
        # This should process the single DML file
        
        click.echo("‚úÖ DML file ingestion completed successfully!")
        
    except Exception as e:
        click.echo(f"‚ùå Error during DML ingestion: {e}", err=True)
        raise


@rag.command()
@click.argument('file_path', type=click.Path(exists=True))
@click.option('-f', '--force', is_flag=True, help='Force re-processing (remove existing data and re-ingest)')
@click.pass_context
@handle_cli_errors
def ingest_python_test(ctx, file_path: str, force: bool):
    """Ingest a Python test file."""
    verbose_echo(ctx, "Ingesting Python test file...")
    
    click.echo(f"üìÑ Python test file: {file_path}")
    click.echo(f"üîÑ Force re-processing: {'Yes' if force else 'No'}")
    
    try:
        if force:
            click.echo("üóëÔ∏è Force mode: Removing existing data for this file...")
            # TODO: Egest/remove existing data for this specific file from database
        
        click.echo("üöÄ Starting Python test file ingestion...")
        click.echo("üß™ Processing test file...")
        click.echo("üß† Generating embeddings...")
        click.echo("üíæ Storing in database...")
        
        # TODO: Integrate with Python test file processing
        # This should process the single Python test file
        
        click.echo("‚úÖ Python test file ingestion completed successfully!")
        
    except Exception as e:
        click.echo(f"‚ùå Error during Python test ingestion: {e}", err=True)
        raise


@rag.command()
@click.argument('file_path', type=click.Path(exists=True))
@click.option('-f', '--force', is_flag=True, help='Force re-processing when content unchanged (regenerate summaries/embeddings)')
@click.pass_context
@handle_cli_errors
def ingest_doc(ctx, file_path: str, force: bool):
    """Ingest a documentation file."""
    from ..backends.factory import get_backend
    from ..services.document_ingest_service import DocumentIngestService
    from ..services.git_service import GitService
    
    verbose_echo(ctx, "Ingesting documentation file...")
    
    click.echo(f"üìÑ Documentation file: {file_path}")
    click.echo(f"üîÑ Force re-processing: {'Yes' if force else 'No'}")
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        # Create git service and document ingest service
        git_service = GitService()
        service = DocumentIngestService(backend, git_service)
        
        # Process the document
        result = service.ingest_document(file_path, force_reprocess=force)
        
        # Display results
        if result['success']:
            if result.get('skipped', False):
                click.echo("‚è≠Ô∏è  Content unchanged - skipped!")
                click.echo(f"üìä Existing file details:")
                click.echo(f"  - File ID: {result['file_id']}")
                click.echo(f"  - Chunks: {result['chunks_created']}")
                click.echo(f"  - Word count: {result['word_count']}")
                click.echo(f"  - Reason: {result.get('reason', 'Content unchanged')}")
                click.echo(f"  - Check time: {result['processing_time']:.2f}s")
                click.echo(f"\nüí° Tip: Use -f to force re-processing (regenerate summaries/embeddings)")
            elif result.get('reprocessed', False):
                click.echo("üîÑ Content unchanged - re-processed with force flag!")
                click.echo(f"üìä Results:")
                click.echo(f"  - File ID: {result['file_id']}")
                click.echo(f"  - Chunks re-created: {result['chunks_created']}")
                click.echo(f"  - Word count: {result['word_count']}")
                click.echo(f"  - Processing time: {result['processing_time']:.2f}s")
                click.echo(f"  - Reason: {result.get('reason', 'Re-processed')}")
            else:
                click.echo("‚úÖ Documentation file ingestion completed successfully!")
                click.echo(f"üìä Results:")
                click.echo(f"  - File ID: {result['file_id']}")
                click.echo(f"  - Chunks created: {result['chunks_created']}")
                click.echo(f"  - Word count: {result['word_count']}")
                click.echo(f"  - Processing time: {result['processing_time']:.2f}s")
                if result.get('new_version'):
                    click.echo(f"  - Status: New version created (content changed)")
        else:
            click.echo(f"‚ùå Ingestion failed: {result['error']}", err=True)
            raise Exception(result['error'])
        
    except Exception as e:
        click.echo(f"‚ùå Error during documentation ingestion: {e}", err=True)
        raise


@rag.command()
@click.argument('file_path', type=click.Path())
@click.option('-f', '--format', type=click.Choice(['json', 'markdown', 'raw']),
              default='json', help='Export format')
@click.pass_context
@handle_cli_errors
def egest_dml(ctx, file_path: str, format: str):
    """Egest (export) DML chunks and metadata to a file."""
    verbose_echo(ctx, "Exporting DML data...")
    
    click.echo(f"üìÑ Export file: {file_path}")
    click.echo(f"üìã Format: {format}")
    
    try:
        # Create output directory if needed
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)
        
        click.echo("üöÄ Starting DML export...")
        click.echo("üìä Querying database for DML sources...")
        click.echo("üìù Processing DML chunks...")
        click.echo("üíæ Writing export file...")
        
        # TODO: Integrate with database export logic
        # Query crawled_pages where source_id LIKE '%dml%'
        # Export chunks with metadata to single file
        
        click.echo(f"‚úÖ DML export completed! Data saved to {file_path}")
        
    except Exception as e:
        click.echo(f"‚ùå Error during DML export: {e}", err=True)
        raise


@rag.command()
@click.argument('file_path', type=click.Path())
@click.option('-f', '--format', type=click.Choice(['json', 'markdown', 'raw']),
              default='json', help='Export format')
@click.pass_context
@handle_cli_errors
def egest_python_test(ctx, file_path: str, format: str):
    """Egest (export) Python test chunks and metadata to a file."""
    verbose_echo(ctx, "Exporting Python test data...")
    
    click.echo(f"üìÑ Export file: {file_path}")
    click.echo(f"üìã Format: {format}")
    
    try:
        # Create output directory if needed
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)
        
        click.echo("üöÄ Starting Python test export...")
        click.echo("üìä Querying database for Python test sources...")
        click.echo("üìù Processing Python test chunks...")
        click.echo("üíæ Writing export file...")
        
        # TODO: Integrate with database export logic
        # Query crawled_pages where source_id LIKE '%python%' AND metadata contains test info
        # Export test chunks with metadata to single file
        
        click.echo(f"‚úÖ Python test export completed! Data saved to {file_path}")
        
    except Exception as e:
        click.echo(f"‚ùå Error during Python test export: {e}", err=True)
        raise


@rag.command()
@click.argument('directory', type=click.Path(exists=True, file_okay=False, dir_okay=True))
@click.option('--pattern', '-p', default='*.md', help='File pattern to match (e.g., *.md,*.html,*.rst)')
@click.option('--recursive/--no-recursive', default=True, help='Search subdirectories recursively')
@click.option('--force', '-f', is_flag=True, help='Force reprocess existing files')
@click.option('--dry-run', is_flag=True, help='Validate files without processing')
@click.option('--parallel', is_flag=True, help='Enable parallel processing (faster but uses more resources)')
@click.option('--workers', '-w', type=int, default=4, help='Number of parallel workers (default: 4)')
@click.pass_context
@handle_cli_errors
def ingest_docs_dir(ctx, directory: str, pattern: str, recursive: bool, 
                    force: bool, dry_run: bool, parallel: bool, workers: int):
    """Ingest multiple documentation files from a directory.
    
    Automatically skips files already in the database (unless --force is used).
    Supports sequential or parallel processing modes.
    """
    from ..backends.factory import get_backend
    from ..services.git_service import GitService
    from ..services.document_ingest_service import DocumentIngestService
    from ..services.bulk_ingest_service import BulkIngestService
    from pathlib import Path
    import time
    
    verbose_echo(ctx, "Starting bulk documentation ingestion...")
    
    dir_path = Path(directory)
    click.echo(f"üìÅ Directory: {dir_path}")
    click.echo(f"üîç Pattern: {pattern}")
    click.echo(f"üìÇ Recursive: {'Yes' if recursive else 'No'}")
    click.echo(f"üîÑ Force reprocess: {'Yes' if force else 'No'}")
    click.echo(f"üß™ Dry run: {'Yes' if dry_run else 'No'}")
    if parallel:
        click.echo(f"‚ö° Parallel processing: Yes ({workers} workers)")
    else:
        click.echo(f"‚ö° Parallel processing: No (sequential)")
    click.echo()
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        # Create services
        git_service = GitService()
        ingest_service = DocumentIngestService(backend, git_service)
        bulk_service = BulkIngestService(backend, git_service, ingest_service)
        
        # Start bulk ingestion
        start_time = time.time()
        mode = "parallel" if parallel else "sequential"
        click.echo(f"üöÄ Starting bulk ingestion ({mode} mode)...")
        click.echo()
        
        progress = bulk_service.ingest_bulk(
            directory=dir_path,
            pattern=pattern,
            recursive=recursive,
            force=force,
            dry_run=dry_run,
            parallel=parallel,
            max_workers=workers
        )
        
        elapsed_time = time.time() - start_time
        
        # Display summary
        click.echo()
        click.echo("=" * 60)
        click.echo("üìä Bulk Ingestion Summary")
        click.echo("=" * 60)
        click.echo(f"Total files discovered: {progress.total_files}")
        click.echo(f"Files processed: {progress.processed}")
        click.echo(f"‚úÖ Succeeded: {progress.succeeded}")
        click.echo(f"‚è≠Ô∏è  Skipped: {progress.skipped}")
        click.echo(f"‚ùå Failed: {progress.failed}")
        click.echo(f"‚è±Ô∏è  Total time: {elapsed_time:.2f}s")
        click.echo(f"üìà Progress: {progress.progress_percent:.1f}%")
        
        if progress.errors:
            click.echo()
            click.echo(f"‚ö†Ô∏è  {len(progress.errors)} errors occurred:")
            for i, error in enumerate(progress.errors[:5], 1):  # Show first 5 errors
                click.echo(f"  {i}. {error.get('file', 'Unknown')}")
                if 'issues' in error:
                    for issue in error['issues']:
                        click.echo(f"     - {issue}")
                elif 'error' in error:
                    click.echo(f"     - {error['error']}")
            
            if len(progress.errors) > 5:
                click.echo(f"  ... and {len(progress.errors) - 5} more errors")
            
            error_report = dir_path / "bulk_ingest_errors.json"
            if error_report.exists():
                click.echo(f"\nüìÑ Full error report: {error_report}")
        
        click.echo("=" * 60)
        
        if dry_run:
            click.echo("\n‚úÖ Dry run completed - no files were actually processed")
        elif progress.failed > 0:
            click.echo("\n‚ö†Ô∏è  Bulk ingestion completed with errors")
        else:
            click.echo("\n‚úÖ Bulk ingestion completed successfully!")
        
    except Exception as e:
        click.echo(f"\n‚ùå Error during bulk ingestion: {e}", err=True)
        raise


@rag.command()
@click.argument('repo_url')
@click.argument('file_path')
@click.option('--commit', help='Query file at specific commit SHA')
@click.option('--timestamp', help='Query file at specific timestamp (ISO format)')
@click.option('--all-versions', is_flag=True, help='Show all versions/history of the file')
@click.option('--limit', type=int, default=100, help='Maximum number of versions to show (with --all-versions)')
@click.option('--json', 'json_output', is_flag=True, help='Output in JSON format')
@click.pass_context
@handle_cli_errors
def query_file(ctx, repo_url: str, file_path: str, commit: Optional[str], 
               timestamp: Optional[str], all_versions: bool, limit: int, json_output: bool):
    """Query a specific file with temporal constraints.
    
    By default, shows current version. Use --all-versions to see file history.
    Use --commit or --timestamp for temporal queries of specific versions.
    """
    from ..backends.factory import get_backend
    from ..services.query_service import QueryService
    from datetime import datetime
    import json
    
    verbose_echo(ctx, "Querying file...")
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        # Create query service
        query_service = QueryService(backend)
        
        # Handle all-versions mode (replaces file-history functionality)
        if all_versions:
            # Get file history
            history = query_service.get_file_history(repo_url, file_path, limit)
            
            if not history:
                click.echo("‚ùå No history found for this file", err=True)
                return
            
            # Output file history
            if json_output:
                # JSON output for history
                output = {
                    'repo_url': repo_url,
                    'file_path': file_path,
                    'version_count': len(history),
                    'query_mode': 'all_versions',
                    'versions': [
                        {
                            'file_id': v.get('id'),
                            'commit_sha': v.get('commit_sha'),
                            'valid_from': v.get('valid_from') if isinstance(v.get('valid_from'), str) else (v.get('valid_from').isoformat() if v.get('valid_from') else None),
                            'valid_until': v.get('valid_until') if isinstance(v.get('valid_until'), str) else (v.get('valid_until').isoformat() if v.get('valid_until') else None),
                            'word_count': v.get('word_count'),
                            'chunk_count': v.get('chunk_count'),
                            'content_hash': v.get('content_hash')
                        }
                        for v in history
                    ]
                }
                click.echo(json.dumps(output, indent=2))
            else:
                # Table format for history
                click.echo("=" * 100)
                click.echo(f"üìú File History: {file_path}")
                click.echo(f"üì¶ Repository: {repo_url}")
                click.echo(f"üìä Total versions: {len(history)}")
                click.echo("=" * 100)
                
                # Header
                click.echo(f"{'Commit':<12} {'Valid From':<20} {'Valid Until':<20} {'Words':<10} {'Chunks':<8}")
                click.echo("-" * 100)
                
                # Rows
                for v in history:
                    commit_sha = str(v.get('commit_sha', ''))[:11]
                    valid_from = str(v.get('valid_from', ''))[:19]
                    valid_until = str(v.get('valid_until') or 'Current')[:19]
                    words = str(v.get('word_count', 0))
                    chunks = str(v.get('chunk_count', 0))
                    
                    click.echo(f"{commit_sha:<12} {valid_from:<20} {valid_until:<20} {words:<10} {chunks:<8}")
                
                click.echo("=" * 100)
            return
        
        # Parse timestamp if provided (for single version queries)
        ts = None
        if timestamp:
            try:
                ts = datetime.fromisoformat(timestamp)
            except ValueError:
                click.echo(f"‚ùå Invalid timestamp format: {timestamp}", err=True)
                click.echo("   Use ISO format: YYYY-MM-DDTHH:MM:SS", err=True)
                return
        
        # Query single file version
        result = query_service.query_file(repo_url, file_path, commit_sha=commit, timestamp=ts)
        
        if not result:
            click.echo("‚ùå File not found", err=True)
            return
        
        # Output single version results  
        if json_output:
            # JSON output for single version
            output = {
                'file_id': result.get('id'),
                'repo_url': repo_url,
                'file_path': result.get('file_path'),
                'query_mode': 'single_version',
                'temporal_constraint': 'commit' if commit else ('timestamp' if timestamp else 'current'),
                'commit_sha': result.get('commit_sha'),
                'content_hash': result.get('content_hash'),
                'file_size': result.get('file_size'),
                'word_count': result.get('word_count'),
                'chunk_count': result.get('chunk_count'),
                'content_type': result.get('content_type'),
                'valid_from': result.get('valid_from') if isinstance(result.get('valid_from'), str) else (result.get('valid_from').isoformat() if result.get('valid_from') else None),
                'valid_until': result.get('valid_until') if isinstance(result.get('valid_until'), str) else (result.get('valid_until').isoformat() if result.get('valid_until') else None),
                'ingested_at': result.get('ingested_at') if isinstance(result.get('ingested_at'), str) else (result.get('ingested_at').isoformat() if result.get('ingested_at') else None)
            }
            click.echo(json.dumps(output, indent=2))
        else:
            # Table format
            click.echo("=" * 60)
            click.echo("üìÑ File Information")
            click.echo("=" * 60)
            click.echo(f"File ID: {result.get('id')}")
            click.echo(f"Repository: {repo_url}")
            click.echo(f"File Path: {result.get('file_path')}")
            click.echo(f"Commit SHA: {result.get('commit_sha')}")
            click.echo(f"Content Hash: {result.get('content_hash', '')[:16]}...")
            click.echo(f"File Size: {result.get('file_size')} bytes")
            click.echo(f"Word Count: {result.get('word_count')}")
            click.echo(f"Chunk Count: {result.get('chunk_count')}")
            click.echo(f"Content Type: {result.get('content_type')}")
            click.echo(f"Valid From: {result.get('valid_from')}")
            click.echo(f"Valid Until: {result.get('valid_until') or 'Current'}")
            click.echo(f"Ingested At: {result.get('ingested_at')}")
            click.echo("=" * 60)
        
    except Exception as e:
        click.echo(f"‚ùå Error querying file: {e}", err=True)
        raise


@rag.command()
@click.option('--repo-url', help='Filter by repository URL')
@click.option('--content-type', type=click.Choice(['documentation', 'code_dml', 'python_test']),
              help='Filter by content type')
@click.option('--current-only/--all-versions', default=True, 
              help='Show only current versions or all versions')
@click.option('--limit', type=int, default=100, help='Maximum number of results')
@click.option('--offset', type=int, default=0, help='Offset for pagination')
@click.option('--json', 'json_output', is_flag=True, help='Output in JSON format')
@click.pass_context
@handle_cli_errors
def list_files(ctx, repo_url: Optional[str], content_type: Optional[str],
               current_only: bool, limit: int, offset: int, json_output: bool):
    """List ingested files with filtering and pagination."""
    from ..backends.factory import get_backend
    from ..services.query_service import QueryService
    import json
    
    verbose_echo(ctx, "Listing files...")
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        # Create query service
        query_service = QueryService(backend)
        
        # List files
        files, total = query_service.list_files(
            repo_url=repo_url,
            content_type=content_type,
            current_only=current_only,
            limit=limit,
            offset=offset
        )
        
        # Enrich files with repository information
        files_with_repos = []
        repo_cache = {}  # Cache to avoid repeated repo lookups
        
        for f in files:
            repo_id = f.get('repo_id')
            repo_info = None
            
            if repo_id:
                # Check cache first
                if repo_id in repo_cache:
                    repo_info = repo_cache[repo_id]
                else:
                    # Fetch repo info and cache it
                    repo_info = backend.get_repository_by_id(repo_id)
                    repo_cache[repo_id] = repo_info
            
            # Add repo info to file
            f_with_repo = f.copy()
            if repo_info:
                f_with_repo['repo_name'] = repo_info.get('repo_name', 'Unknown')
                f_with_repo['repo_url'] = repo_info.get('repo_url', 'Unknown')
            else:
                f_with_repo['repo_name'] = 'Unknown'
                f_with_repo['repo_url'] = 'Unknown'
            
            files_with_repos.append(f_with_repo)
        
        files = files_with_repos
        
        if json_output:
            # JSON output
            output = {
                'total': total,
                'limit': limit,
                'offset': offset,
                'count': len(files),
                'files': [
                    {
                        'file_id': f.get('id'),
                        'file_path': f.get('file_path'),
                        'repo_name': f.get('repo_name'),
                        'repo_url': f.get('repo_url'),
                        'commit_sha': f.get('commit_sha'),
                        'content_type': f.get('content_type'),
                        'word_count': f.get('word_count'),
                        'chunk_count': f.get('chunk_count'),
                        'valid_from': f.get('valid_from') if isinstance(f.get('valid_from'), str) else (f.get('valid_from').isoformat() if f.get('valid_from') else None),
                        'valid_until': f.get('valid_until') if isinstance(f.get('valid_until'), str) else (f.get('valid_until').isoformat() if f.get('valid_until') else None)
                    }
                    for f in files
                ]
            }
            click.echo(json.dumps(output, indent=2))
        else:
            # List format
            click.echo(f"\nüìö Files List (showing {len(files)} of {total} total)")
            click.echo("=" * 80)
            
            if not files:
                click.echo("No files found matching the criteria")
            else:
                for i, f in enumerate(files, 1):
                    click.echo(f"\nüìÑ File {i}:")
                    click.echo(f"  ID: {f.get('id', 'Unknown')}")
                    click.echo(f"  Repository: {f.get('repo_url', 'Unknown')}")
                    click.echo(f"  Path: {f.get('file_path', 'Unknown')}")
                    click.echo(f"  Type: {f.get('content_type', 'Unknown')}")
                    click.echo(f"  Chunks: {f.get('chunk_count', 0)}")
                    click.echo(f"  Words: {f.get('word_count', 0)}")
                    click.echo(f"  Commit: {f.get('commit_sha', 'Unknown')}")
                    
                    # Show temporal info if available
                    valid_from = f.get('valid_from')
                    valid_until = f.get('valid_until')
                    if valid_from:
                        click.echo(f"  Valid From: {valid_from}")
                    if valid_until:
                        click.echo(f"  Valid Until: {valid_until}")
                    elif valid_from:
                        click.echo(f"  Valid Until: Current")
            
            click.echo("\n" + "=" * 80)
            
            # Pagination info
            if total > limit:
                pages = (total + limit - 1) // limit
                current_page = (offset // limit) + 1
                click.echo(f"Page {current_page} of {pages} | Use --offset and --limit for pagination")
        
    except Exception as e:
        click.echo(f"‚ùå Error listing files: {e}", err=True)
        raise


@rag.command()
@click.argument('file_id', type=int)
@click.option('--json', 'json_output', is_flag=True, help='Output in JSON format')
@click.pass_context
@handle_cli_errors
def list_chunks(ctx, file_id: int, json_output: bool):
    """List all chunks for a specific file."""
    from ..backends.factory import get_backend
    from ..services.query_service import QueryService
    import json
    
    verbose_echo(ctx, "Listing chunks...")
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        # Create query service
        query_service = QueryService(backend)
        
        # Get chunks
        chunks = query_service.get_file_chunks(file_id)
        
        if not chunks:
            click.echo(f"‚ùå No chunks found for file ID {file_id}", err=True)
            return
        
        if json_output:
            # JSON output
            output = {
                'file_id': file_id,
                'chunk_count': len(chunks),
                'chunks': [
                    {
                        'chunk_id': c.get('id'),
                        'chunk_number': c.get('chunk_number'),
                        'content_preview': c.get('content', '')[:100] + '...' if len(c.get('content', '')) > 100 else c.get('content', ''),
                        'word_count': c.get('metadata', {}).get('word_count'),
                        'has_code': c.get('metadata', {}).get('has_code'),
                        'summary': c.get('summary')
                    }
                    for c in chunks
                ]
            }
            click.echo(json.dumps(output, indent=2))
        else:
            # Table format
            click.echo("=" * 100)
            click.echo(f"üì¶ Chunks for File ID: {file_id}")
            click.echo(f"üìä Total chunks: {len(chunks)}")
            click.echo("=" * 100)
            
            # Header
            click.echo(f"{'#':<5} {'Words':<8} {'Code':<6} {'Summary Preview':<70}")
            click.echo("-" * 100)
            
            # Rows
            for c in chunks:
                num = str(c.get('chunk_number', ''))
                words = str(c.get('metadata', {}).get('word_count', 0))
                has_code = 'Yes' if c.get('metadata', {}).get('has_code') else 'No'
                summary = str(c.get('summary', ''))[:69] if c.get('summary') else '-'
                
                click.echo(f"{num:<5} {words:<8} {has_code:<6} {summary:<70}")
            
            click.echo("=" * 100)
        
    except Exception as e:
        click.echo(f"‚ùå Error listing chunks: {e}", err=True)
        raise


@rag.command()
@click.argument('directory', type=click.Path(exists=True, file_okay=False, dir_okay=True))
@click.option('--pattern', '-p', default='*.md', help='File pattern to match (e.g., *.md,*.html,*.rst)')
@click.option('--recursive/--no-recursive', default=True, help='Search subdirectories recursively')
@click.option('--commit', help='Remove only this specific commit version for all files (default: remove all versions)')
@click.option('--dry-run', is_flag=True, help='Show what would be removed without actually removing')
@click.option('--confirm', '-c', is_flag=True, help='Skip confirmation prompt')
@click.option('--parallel', is_flag=True, help='Enable parallel processing (faster but uses more resources)')
@click.option('--workers', '-w', type=int, default=4, help='Number of parallel workers (default: 4)')
@click.pass_context
@handle_cli_errors
def egest_docs_dir(ctx, directory: str, pattern: str, recursive: bool, commit: Optional[str], 
                   dry_run: bool, confirm: bool, parallel: bool, workers: int):
    """Egest (remove) multiple documentation files and their chunks from the database.
    
    By default, removes ALL versions of each file. Use --commit to remove only a specific version.
    Supports sequential or parallel processing modes.
    """
    from ..backends.factory import get_backend
    from ..services.git_service import GitService
    from ..services.bulk_ingest_service import BulkIngestService
    from pathlib import Path
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import time
    import threading
    
    verbose_echo(ctx, "Starting bulk documentation removal...")
    
    dir_path = Path(directory)
    click.echo(f"üìÅ Directory: {dir_path}")
    click.echo(f"üîç Pattern: {pattern}")
    click.echo(f"üìÇ Recursive: {'Yes' if recursive else 'No'}")
    if commit:
        click.echo(f"üîñ Commit filter: {commit}")
    else:
        click.echo(f"üîñ Remove all versions: Yes")
    click.echo(f"üß™ Dry run: {'Yes' if dry_run else 'No'}")
    if parallel:
        click.echo(f"‚ö° Parallel processing: Yes ({workers} workers)")
    else:
        click.echo(f"‚ö° Parallel processing: No (sequential)")
    click.echo()
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        # Create services
        git_service = GitService()
        bulk_service = BulkIngestService(backend, git_service, None)  # Don't need ingest service for removal
        
        # Discover files matching pattern
        click.echo("üîç Discovering files...")
        files = bulk_service.discover_files(dir_path, pattern, recursive)
        
        if not files:
            click.echo("üì≠ No files found matching the pattern.")
            return
        
        click.echo(f"üìä Found {len(files)} files to process")
        
        # Get git repository info for path normalization
        git_info = git_service.detect_repository(dir_path)
        
        # Track removal statistics
        removal_stats = {
            'total_files': len(files),
            'processed': 0,
            'succeeded': 0,
            'failed': 0,
            'not_found': 0,
            'total_chunks': 0,
            'total_words': 0,
            'errors': []
        }
        
        # Thread lock for stats updates
        stats_lock = threading.Lock()
        
        def process_single_file(file_path: Path) -> dict:
            """Process a single file for removal."""
            # Calculate normalized path
            if git_info:
                normalized_path = git_info.get_relative_path(file_path.resolve())
                repo = backend.get_repository(git_info.repo_url)
                repo_id = repo['id'] if repo else None
            else:
                normalized_path = str(file_path)
                repo_id = None
            
            # Get file versions
            all_versions = []
            if repo_id:
                all_versions = backend.get_file_history(repo_id, normalized_path, limit=1000)
            
            if not all_versions:
                # Fallback: check if current version exists
                try:
                    from .utils import calculate_file_hash
                    if file_path.exists():
                        file_hash = calculate_file_hash(str(file_path))
                        existing = backend.check_file_exists(normalized_path, file_hash)
                        if existing:
                            all_versions = [existing]
                except Exception:
                    pass
            
            if not all_versions:
                return {
                    'status': 'not_found',
                    'file_path': str(file_path),
                    'normalized_path': normalized_path,
                    'chunks': 0,
                    'words': 0
                }
            
            # Filter by commit if specified
            versions_to_remove = all_versions
            if commit:
                versions_to_remove = [v for v in all_versions if v.get('commit_sha', '').startswith(commit)]
                if not versions_to_remove:
                    return {
                        'status': 'commit_not_found',
                        'file_path': str(file_path),
                        'normalized_path': normalized_path,
                        'commit': commit,
                        'available_commits': [v.get('commit_sha', 'Unknown')[:8] for v in all_versions[:5]],
                        'chunks': 0,
                        'words': 0
                    }
            
            # Calculate totals
            total_chunks = sum(v.get('chunk_count', 0) for v in versions_to_remove)
            total_words = sum(v.get('word_count', 0) for v in versions_to_remove)
            
            if dry_run:
                return {
                    'status': 'would_remove',
                    'file_path': str(file_path),
                    'normalized_path': normalized_path,
                    'versions': len(versions_to_remove),
                    'chunks': total_chunks,
                    'words': total_words
                }
            
            # Actually remove the file(s)
            try:
                if commit:
                    success = backend.remove_file_version(normalized_path, commit)
                else:
                    success = backend.remove_file_data(normalized_path)
                
                if success:
                    return {
                        'status': 'removed',
                        'file_path': str(file_path),
                        'normalized_path': normalized_path,
                        'versions': len(versions_to_remove),
                        'chunks': total_chunks,
                        'words': total_words
                    }
                else:
                    return {
                        'status': 'failed',
                        'file_path': str(file_path),
                        'normalized_path': normalized_path,
                        'error': 'Backend removal failed',
                        'chunks': total_chunks,
                        'words': total_words
                    }
            except Exception as e:
                return {
                    'status': 'error',
                    'file_path': str(file_path),
                    'normalized_path': normalized_path,
                    'error': str(e),
                    'chunks': total_chunks,
                    'words': total_words
                }
        
        # Show preview and get confirmation unless --confirm or --dry-run
        if not dry_run and not confirm:
            click.echo("‚ö†Ô∏è  This will permanently remove files from the database:")
            click.echo(f"   - Directory: {dir_path}")
            click.echo(f"   - Pattern: {pattern}")
            click.echo(f"   - Files to process: {len(files)}")
            if commit:
                click.echo(f"   - Commit filter: {commit}")
            else:
                click.echo(f"   - All versions of each file")
            
            if not click.confirm("\nAre you sure you want to proceed?"):
                click.echo("‚ùå Operation cancelled")
                return
        
        # Process files
        start_time = time.time()
        mode = "parallel" if parallel else "sequential"
        click.echo(f"üöÄ Starting bulk removal ({mode} mode)...")
        click.echo()
        
        if parallel:
            # Parallel processing
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_file = {
                    executor.submit(process_single_file, file_path): file_path
                    for file_path in files
                }
                
                for future in as_completed(future_to_file):
                    file_path = future_to_file[future]
                    try:
                        result = future.result()
                        
                        with stats_lock:
                            removal_stats['processed'] += 1
                            
                            if result['status'] == 'removed':
                                removal_stats['succeeded'] += 1
                                removal_stats['total_chunks'] += result['chunks']
                                removal_stats['total_words'] += result['words']
                            elif result['status'] in ['would_remove']:
                                removal_stats['succeeded'] += 1  # For dry run
                                removal_stats['total_chunks'] += result['chunks']
                                removal_stats['total_words'] += result['words']
                            elif result['status'] in ['not_found', 'commit_not_found']:
                                removal_stats['not_found'] += 1
                            else:
                                removal_stats['failed'] += 1
                                removal_stats['errors'].append(result)
                            
                            # Progress update
                            if removal_stats['processed'] % 10 == 0:
                                progress = removal_stats['processed'] / removal_stats['total_files'] * 100
                                click.echo(f"Progress: {progress:.1f}% ({removal_stats['processed']}/{removal_stats['total_files']})")
                    
                    except Exception as e:
                        with stats_lock:
                            removal_stats['processed'] += 1
                            removal_stats['failed'] += 1
                            removal_stats['errors'].append({
                                'status': 'exception',
                                'file_path': str(file_path),
                                'error': str(e)
                            })
        else:
            # Sequential processing
            for i, file_path in enumerate(files, 1):
                try:
                    result = process_single_file(file_path)
                    
                    removal_stats['processed'] += 1
                    
                    if result['status'] == 'removed':
                        removal_stats['succeeded'] += 1
                        removal_stats['total_chunks'] += result['chunks']
                        removal_stats['total_words'] += result['words']
                    elif result['status'] in ['would_remove']:
                        removal_stats['succeeded'] += 1  # For dry run
                        removal_stats['total_chunks'] += result['chunks']
                        removal_stats['total_words'] += result['words']
                    elif result['status'] in ['not_found', 'commit_not_found']:
                        removal_stats['not_found'] += 1
                    else:
                        removal_stats['failed'] += 1
                        removal_stats['errors'].append(result)
                    
                    # Progress update
                    if i % 10 == 0:
                        progress = i / removal_stats['total_files'] * 100
                        click.echo(f"Progress: {progress:.1f}% ({i}/{removal_stats['total_files']})")
                
                except Exception as e:
                    removal_stats['processed'] += 1
                    removal_stats['failed'] += 1
                    removal_stats['errors'].append({
                        'status': 'exception',
                        'file_path': str(file_path),
                        'error': str(e)
                    })
        
        elapsed_time = time.time() - start_time
        
        # Display summary
        click.echo()
        click.echo("=" * 60)
        if dry_run:
            click.echo("üìä Bulk Removal Preview (Dry Run)")
        else:
            click.echo("üìä Bulk Removal Summary")
        click.echo("=" * 60)
        click.echo(f"Total files discovered: {removal_stats['total_files']}")
        click.echo(f"Files processed: {removal_stats['processed']}")
        if dry_run:
            click.echo(f"üìã Would remove: {removal_stats['succeeded']}")
        else:
            click.echo(f"‚úÖ Removed: {removal_stats['succeeded']}")
        click.echo(f"üì≠ Not found in DB: {removal_stats['not_found']}")
        click.echo(f"‚ùå Failed: {removal_stats['failed']}")
        click.echo(f"üì¶ Total chunks affected: {removal_stats['total_chunks']}")
        click.echo(f"üìù Total words affected: {removal_stats['total_words']}")
        click.echo(f"‚è±Ô∏è  Total time: {elapsed_time:.2f}s")
        
        if removal_stats['errors']:
            click.echo()
            click.echo(f"‚ö†Ô∏è  {len(removal_stats['errors'])} errors occurred:")
            for i, error in enumerate(removal_stats['errors'][:5], 1):  # Show first 5 errors
                click.echo(f"  {i}. {error.get('file_path', 'Unknown')}: {error.get('error', error.get('status', 'Unknown error'))}")
            
            if len(removal_stats['errors']) > 5:
                click.echo(f"  ... and {len(removal_stats['errors']) - 5} more errors")
        
        click.echo("=" * 60)
        
        if dry_run:
            click.echo("\n‚úÖ Dry run completed - no files were actually removed")
        elif removal_stats['failed'] > 0:
            click.echo("\n‚ö†Ô∏è  Bulk removal completed with errors")
        else:
            click.echo("\n‚úÖ Bulk removal completed successfully!")
        
    except Exception as e:
        click.echo(f"\n‚ùå Error during bulk removal: {e}", err=True)
        raise


@rag.command()
@click.option('--dry-run', is_flag=True, help='Show what would be removed without actually removing')
@click.option('-c', '--confirm', is_flag=True, help='Skip confirmation prompt')
@click.pass_context
@handle_cli_errors
def cleanup_orphans(ctx, dry_run: bool, confirm: bool):
    """Clean up orphaned chunks that reference non-existent files.
    
    Finds and removes chunks in the database that have no corresponding file records.
    This can happen when file removal operations are incomplete or interrupted.
    """
    from ..backends.factory import get_backend
    
    verbose_echo(ctx, "Starting orphaned chunks cleanup...")
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        # Detect backend type for display
        backend_type = backend.get_backend_name()
        click.echo(f"üîç Analyzing {backend_type} database...")
        
        # Run analysis (dry run first to get stats)
        analysis = backend.cleanup_orphaned_chunks(dry_run=True)
        
        # Display analysis results
        click.echo(f"üìä Database Analysis:")
        click.echo(f"   Total chunks: {analysis['total_chunks']}")
        click.echo(f"   Valid files: {analysis['valid_files']}")
        click.echo(f"   Orphaned chunks: {analysis['orphaned_chunks']}")
        
        if analysis['orphan_groups']:
            click.echo(f"\nüìã Orphaned chunks by file:")
            for file_id, info in analysis['orphan_groups'].items():
                click.echo(f"   File ID {file_id}: {info['count']} chunks ({info['url']})")
        
        if analysis['orphaned_chunks'] == 0:
            click.echo("\n‚úÖ No orphaned chunks found - database is clean!")
            return
        
        if dry_run:
            click.echo(f"\nüß™ Dry run - would remove {analysis['orphaned_chunks']} orphaned chunks")
            return
        
        # Confirm cleanup unless --confirm flag
        if not confirm:
            click.echo(f"\n‚ö†Ô∏è  This will permanently remove {analysis['orphaned_chunks']} orphaned chunks:")
            for file_id, info in analysis['orphan_groups'].items():
                click.echo(f"   - File ID {file_id}: {info['count']} chunks")
            
            if not click.confirm("\nAre you sure you want to proceed?"):
                click.echo("‚ùå Operation cancelled")
                return
        
        # Perform actual cleanup
        click.echo(f"\nüßπ Cleaning up {analysis['orphaned_chunks']} orphaned chunks...")
        result = backend.cleanup_orphaned_chunks(dry_run=False)
        
        # Display results
        click.echo(f"‚úÖ Cleanup completed!")
        click.echo(f"   Chunks before: {result['total_chunks']}")
        click.echo(f"   Chunks after: {result['total_chunks'] - result['removed']}")
        click.echo(f"   Removed: {result['removed']}")
            
    except Exception as e:
        click.echo(f"\n‚ùå Error during cleanup: {e}", err=True)
        raise


@rag.command()
@click.argument('file_path', type=click.Path())
@click.option('--commit', help='Remove only this specific commit version (default: remove all versions)')
@click.option('-c', '--confirm', is_flag=True, help='Skip confirmation prompt')
@click.pass_context
@handle_cli_errors
def egest_doc(ctx, file_path: str, commit: Optional[str], confirm: bool):
    """Egest (remove) a documentation file and its chunks from the database.
    
    By default, removes ALL versions of the file. Use --commit to remove only a specific version.
    """
    from ..backends.factory import get_backend
    from ..services.git_service import GitService
    from pathlib import Path
    
    verbose_echo(ctx, "Removing documentation file from database...")
    
    click.echo(f"üìÑ Documentation file: {file_path}")
    if commit:
        click.echo(f"üîñ Commit filter: {commit}")
    
    try:
        # Get backend
        backend_name = ctx.obj.get('db_backend')
        backend = get_backend(backend_name)
        
        if not backend.is_connected():
            click.echo("‚ùå Database not connected", err=True)
            return
        
        # Normalize file path using git service (same as ingest)
        git_service = GitService()
        git_info = git_service.detect_repository(Path(file_path))
        
        normalized_path = file_path
        repo_id = None
        
        if git_info:
            # Get relative path from git root (same as ingest does)
            abs_file_path = Path(file_path).resolve()
            normalized_path = git_info.get_relative_path(abs_file_path)
            
            # Get repository ID
            repo = backend.get_repository(git_info.repo_url)
            if repo:
                repo_id = repo['id']
        
        # Get all versions of this file
        all_versions = []
        if repo_id:
            all_versions = backend.get_file_history(repo_id, normalized_path, limit=1000)
        
        if not all_versions:
            # Fallback: check if current version exists
            try:
                from .utils import calculate_file_hash
                file_hash = calculate_file_hash(file_path)
                existing = backend.check_file_exists(normalized_path, file_hash)
                if existing:
                    all_versions = [existing]
            except FileNotFoundError:
                click.echo(f"‚ùå File not found: {file_path}", err=True)
                return
            except Exception as e:
                click.echo(f"‚ùå Error reading file: {e}", err=True)
                return
        
        if not all_versions:
            click.echo(f"‚ö†Ô∏è  File not found in database:")
            click.echo(f"   - Path: {normalized_path}")
            click.echo(f"   - File may have been modified or never ingested")
            return
        
        # Filter by commit if specified
        versions_to_remove = all_versions
        if commit:
            versions_to_remove = [v for v in all_versions if v.get('commit_sha', '').startswith(commit)]
            if not versions_to_remove:
                click.echo(f"‚ùå No version found for commit: {commit}")
                click.echo(f"\nüìú Available commits:")
                for v in all_versions[:10]:
                    commit_sha = v.get('commit_sha', 'Unknown')
                    valid_from = str(v.get('valid_from', 'Unknown'))[:19]
                    click.echo(f"   - {commit_sha[:8]} from {valid_from}")
                return
        
        # Calculate totals across versions to remove
        total_chunks = sum(v.get('chunk_count', 0) for v in versions_to_remove)
        total_words = sum(v.get('word_count', 0) for v in versions_to_remove)
        
        # Confirmation prompt unless --confirm flag is used
        if not confirm:
            if commit:
                click.echo(f"‚ö†Ô∏è  This will permanently remove specific version:")
                click.echo(f"   - File path: {normalized_path}")
                click.echo(f"   - Commit: {versions_to_remove[0].get('commit_sha', 'Unknown')[:8]}")
                click.echo(f"   - Chunks: {total_chunks}")
                click.echo(f"   - Words: {total_words}")
            else:
                click.echo(f"‚ö†Ô∏è  This will permanently remove ALL versions:")
                click.echo(f"   - File path: {normalized_path}")
                click.echo(f"   - Total versions: {len(versions_to_remove)}")
                click.echo(f"   - Total chunks: {total_chunks}")
                click.echo(f"   - Total words: {total_words}")
                
                if len(versions_to_remove) > 1:
                    click.echo(f"\n   üìú Version history:")
                    for i, v in enumerate(versions_to_remove[:5], 1):  # Show first 5
                        commit_sha = str(v.get('commit_sha', 'Unknown'))[:8]
                        valid_from = str(v.get('valid_from', 'Unknown'))[:19]
                        click.echo(f"      {i}. Commit {commit_sha} from {valid_from} ({v.get('chunk_count', 0)} chunks)")
                    if len(versions_to_remove) > 5:
                        click.echo(f"      ... and {len(versions_to_remove) - 5} more versions")
            
            if not click.confirm("\nAre you sure you want to proceed?"):
                click.echo("‚ùå Operation cancelled")
                return
        
        # Remove based on whether we're removing all or specific version
        if commit:
            click.echo(f"üóëÔ∏è  Removing specific version from database...")
            # Remove specific version by commit
            success = backend.remove_file_version(normalized_path, commit)
        else:
            click.echo(f"üóëÔ∏è  Removing all versions from database...")
            # Remove all versions
            success = backend.remove_file_data(normalized_path)
        
        if success:
            click.echo("‚úÖ Documentation file removed successfully!")
            click.echo(f"üìä Removed:")
            click.echo(f"  - File path: {normalized_path}")
            if commit:
                click.echo(f"  - Commit: {commit}")
            else:
                click.echo(f"  - Versions removed: {len(versions_to_remove)}")
            click.echo(f"  - Total chunks: {total_chunks}")
            click.echo(f"  - Total words: {total_words}")
        else:
            click.echo("‚ö†Ô∏è  File not found in database or already removed")
        
    except Exception as e:
        click.echo(f"‚ùå Error during file removal: {e}", err=True)
        raise


